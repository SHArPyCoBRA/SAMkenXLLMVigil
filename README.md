# vigil-llm
Detect prompt injections, jailbreaks, and other potentially risky Large Language Model (LLM) inputs
